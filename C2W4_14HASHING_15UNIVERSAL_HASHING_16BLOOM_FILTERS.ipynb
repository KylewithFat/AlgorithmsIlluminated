{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIV. HASHING: THE BASICS \n",
    "You can think about the purpose of the hash table is to maintain a possibly evolving set of things. Where of course the set of things that you're maintaining, you know, will vary with the application. It can be any number of things. So if you're running an e-commerce website, maybe you're keeping track of transactions. You know, again, maybe you're keeping track of people, like for example, your friends and various data about them. So maybe you're keeping track of I-P addresses, for example if you wanna know, who was, were there unique visitors to your websites. And so on. So a little bit more formally, you know, the basic operations, you need to be able to insert stuff into a hash table. In many, but not all applications, you need to be able to delete stuff as well. And typically the most important operation is look-up. And for all these three operation you do it in a key based way. Where as usual a key should just be a unique identifier for the record that you're concerned with.\n",
    "\n",
    "The caveat is that, unlike most of the problems that we've solved in this course, hash tables don't enjoy worst case guarantees. You cannot say for a given hash table that for every possible data set you're gonna get cost and time. What's true is that for non-pathological data, you will get cost and time operations in a properly implemented hash table.\n",
    "\n",
    "***\n",
    "##### Properties of Hash function:\n",
    "1. Should lead to good performance (i.e. should spread data out)\n",
    "2. Should be easy to evaluate / fast to calculate\n",
    "<br>\n",
    " And in particular good hash functions that have the two properties we identified above. But I have to warn you, if you ask ten different, you know, serious hardcore programmers, you know, about their approach to designing hash functions, you're likely to get ten somewhat different answers. So the design of hash functions is a tricky topic, and, it's as much art as science at this point.\n",
    "*** \n",
    "\n",
    "##### Resolving Collisions: \n",
    "    1. Chaining  \n",
    "    2. Open addressing(linear probing and double hashing)\n",
    "\n",
    "***\n",
    "\n",
    "## Quick and Dirty Hash Function\n",
    "In particular if you just need a hash function, and you need a quick and dirty one, you don't want to spend too much time on it. The method that I'm going to talk about below is a common way of doing it. On the other hand, if you're designing a hash function for some really mission-critical code, you should learn more than what I'm gonna tell you about below. So you, you should do more research about what are the best hash functions, what's the state of the art, if you have a super important hash function.\n",
    "\n",
    "There are standard methods for doing that, it's easy to find resources to, to give you example code for converting strings to integers you know, I'll just say one or two sentences about it. So you know each character in a string it is easy to regard as a number in various ways. Either you know just say it is ASCII, well ASCII code then you just have to aggregate all of the different numbers, one number per character into some overall number and so one thing you can do is you can iterate over the characters one at a time. You can keep a running sum. And with each character, you can multiply the running sum by some constant, and then add the new letter to it, and then, if you need to, take a module list to prevent overflow.\n",
    "\n",
    "![Quick and Dirty Hash Function](images/4_quick_and_dirty_hash_function.png)\n",
    "\n",
    "\n",
    "# XV. UNIVERSAL HASHING \n",
    "If we have our set h that we know exactly what it is. What does it mean that it's universal? It means for each pair of distinct keys, for example for each pair of IP addresses, the probability that a random hash function from our family script h causes a collision, maps these two IP addresses to the same bucket should be no worse than with perfectly random hashing. So no worse than 1/n where n is the number of buckets, say like 997.\n",
    "So  if you draw a hash function uniformly at random from a universal family of hash functions, then you're guaranteed expected constant time for all of the supported operations. \n",
    "\n",
    "![UNIVERSAL HASHING](images/5_universal_hashing.png)\n",
    "\n",
    "\n",
    "So hash tables support various operations, Insert, Delete and Lookup. But really if we can just bound a running time of an unsuccessful lookup, that's going to be enough to bound the running time of all of these operations. \n",
    "\n",
    "So, as long as you have a hash function which you can compute quickly in constant time. And as long as you keep the load under control so the number of buckets is commensurate with the size of the data set that you're storing. That's why, universal hash functions in a hash table with chaining guarantee expected constant time performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
